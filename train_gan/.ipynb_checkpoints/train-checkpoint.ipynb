{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f049f13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from tqdm import tqdm, trange\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision import models\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c0aa883",
   "metadata": {},
   "outputs": [],
   "source": [
    "from net import Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "266202a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StyleLoader():\n",
    "    def __init__(self, style_folder, style_size, cuda=True):\n",
    "        self.folder = style_folder\n",
    "        self.style_size = style_size\n",
    "        self.files = os.listdir(style_folder)\n",
    "        self.cuda = cuda\n",
    "    \n",
    "    def get(self, i):\n",
    "        idx = i%len(self.files)\n",
    "        filepath = os.path.join(self.folder, self.files[idx])\n",
    "        style = tensor_load_rgbimage(filepath, self.style_size)    \n",
    "        style = style.unsqueeze(0)\n",
    "        style = preprocess_batch(style)\n",
    "        if self.cuda:\n",
    "            style = style.cuda()\n",
    "        style_v = Variable(style, requires_grad=False)\n",
    "        return style_v\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "903663fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_matrix(y):\n",
    "    (b, ch, h, w) = y.size()\n",
    "    features = y.view(b, ch, w * h)\n",
    "    features_t = features.transpose(1, 2)\n",
    "    gram = features.bmm(features_t) / (ch * h * w)\n",
    "    return gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09dc3554",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vgg16(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Vgg16, self).__init__()\n",
    "        self.conv1_1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv1_2 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.conv2_1 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2_2 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.conv3_1 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3_2 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3_3 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.conv4_1 = nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv4_2 = nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv4_3 = nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.conv5_1 = nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv5_2 = nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv5_3 = nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    def forward(self, X):\n",
    "        h = F.relu(self.conv1_1(X))\n",
    "        h = F.relu(self.conv1_2(h))\n",
    "        relu1_2 = h\n",
    "        h = F.max_pool2d(h, kernel_size=2, stride=2)\n",
    "\n",
    "        h = F.relu(self.conv2_1(h))\n",
    "        h = F.relu(self.conv2_2(h))\n",
    "        relu2_2 = h\n",
    "        h = F.max_pool2d(h, kernel_size=2, stride=2)\n",
    "\n",
    "        h = F.relu(self.conv3_1(h))\n",
    "        h = F.relu(self.conv3_2(h))\n",
    "        h = F.relu(self.conv3_3(h))\n",
    "        relu3_3 = h\n",
    "        h = F.max_pool2d(h, kernel_size=2, stride=2)\n",
    "\n",
    "        h = F.relu(self.conv4_1(h))\n",
    "        h = F.relu(self.conv4_2(h))\n",
    "        h = F.relu(self.conv4_3(h))\n",
    "        relu4_3 = h\n",
    "\n",
    "        return [relu1_2, relu2_2, relu3_3, relu4_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c94cad49",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_nc=3, output_nc=3, ngf=64, norm_layer=nn.InstanceNorm2d, n_blocks=6, gpu_ids=[]):\n",
    "        super(Net, self).__init__()\n",
    "        self.gpu_ids = gpu_ids\n",
    "        self.gram = GramMatrix()\n",
    "\n",
    "        block = Bottleneck\n",
    "        upblock = UpBottleneck\n",
    "        expansion = 4\n",
    "\n",
    "        model1 = []\n",
    "        model1 += [ConvLayer(input_nc, 64, kernel_size=7, stride=1),\n",
    "                            norm_layer(64),\n",
    "                            nn.ReLU(inplace=True),\n",
    "                            block(64, 32, 2, 1, norm_layer),\n",
    "                            block(32*expansion, ngf, 2, 1, norm_layer)]\n",
    "        self.model1 = nn.Sequential(*model1)\n",
    "\n",
    "        model = []\n",
    "        self.ins = Inspiration(ngf*expansion)\n",
    "        model += [self.model1]\n",
    "        model += [self.ins]    \n",
    "\n",
    "        for i in range(n_blocks):\n",
    "            model += [block(ngf*expansion, ngf, 1, None, norm_layer)]\n",
    "        \n",
    "        model += [upblock(ngf*expansion, 32, 2, norm_layer),\n",
    "                            upblock(32*expansion, 16, 2, norm_layer),\n",
    "                            norm_layer(16*expansion),\n",
    "                            nn.ReLU(inplace=True),\n",
    "                            ConvLayer(16*expansion, output_nc, kernel_size=7, stride=1)]\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def setTarget(self, Xs):\n",
    "        F = self.model1(Xs)\n",
    "        G = self.gram(F)\n",
    "        self.ins.setTarget(G)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "375d9b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(inp, title=None, plt_ax=None, default=False):\n",
    "    if plt_ax is None:\n",
    "        plt_ax = plt.gca()\n",
    "    if title is None and type(inp) is tuple:\n",
    "        inp, title = inp\n",
    "    if type(inp) is tuple:\n",
    "        inp = inp[0]\n",
    "    if type(inp) is Image.Image:\n",
    "        inp = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])(inp)\n",
    "    if type(inp) is torch.Tensor and inp.is_cuda:\n",
    "        inp = inp.cpu()\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    \n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt_ax.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt_ax.set_title(title)\n",
    "    plt_ax.grid(False)\n",
    "    \n",
    "def grid_show(data, max_images=9, random=False, cols=3):\n",
    "    imcount = min(len(data), max_images) if max_images > 0 else len(data)\n",
    "    rows = int(np.ceil(imcount / cols))\n",
    "    fig, axs = plt.subplots(ncols=cols, nrows=rows, figsize=(15, rows * 5), sharey=True, sharex=True)\n",
    "    axs = axs.flatten()\n",
    "    idxs = range(imcount)\n",
    "    if random:\n",
    "        idxs = np.random.choice(len(data), size=imcount)\n",
    "    for ax, i in enumerate(idxs):\n",
    "        item = data[i]\n",
    "        if hasattr(data, 'classes') and type(item) is tuple and len(item) == 2 and type(item[1]) is int:\n",
    "            item = item[0], data.classes[item[1]]\n",
    "        imshow(item, plt_ax=axs[ax])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c910751a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-01-23 03:07:16--  http://images.cocodataset.org/zips/train2014.zip\n",
      "Распознаётся images.cocodataset.org (images.cocodataset.org)… 52.217.235.201, 52.217.164.217, 52.217.100.68, ...\n",
      "Подключение к images.cocodataset.org (images.cocodataset.org)|52.217.235.201|:80... соединение установлено.\n",
      "HTTP-запрос отправлен. Ожидание ответа… 200 OK\n",
      "Длина: 13510573713 (13G) [application/zip]\n",
      "Сохранение в: «train2014.zip»\n",
      "\n",
      "train2014.zip         2%[                    ] 339,28M  2,19MB/s    ост 68m 4s ^C\n"
     ]
    }
   ],
   "source": [
    "!wget http://images.cocodataset.org/zips/train2014.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34bd52a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs=3, batch_size=5, content_weight = 1.0, style_weight=100000.0):\n",
    "    transform = transforms.Compose([transforms.Resize(256),\n",
    "                                    transforms.CenterCrop(256),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "    train_dataset = datasets.ImageFolder('dataset_content', transform)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "\n",
    "    style_model = Net()\n",
    "    \n",
    "    if os.path.exists('msgnet_checkpoint.pth'):\n",
    "        style_model.load_state_dict(torch.load('msgnet_checkpoint.pth'))\n",
    "        \n",
    "    style_model = style_model.cuda()\n",
    "    style_model.train()\n",
    "    \n",
    "    optimizer = Adam(style_model.parameters(), 1e-3)\n",
    "    mse_loss = torch.nn.MSELoss()\n",
    "\n",
    "    vgg = Vgg16Featurer().cuda()\n",
    "\n",
    "    style_loader = StyleLoader('dataset_style', 512, cuda=True)\n",
    "\n",
    "    tbar = trange(epochs)\n",
    "    for e in tbar:\n",
    "        style_model.train()\n",
    "        agg_content_loss = 0.\n",
    "        agg_style_loss = 0.\n",
    "        count = 0\n",
    "        for batch_id, (x, _) in enumerate(train_loader):\n",
    "            n_batch = len(x)\n",
    "            count += n_batch\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            x = x.cuda()\n",
    "\n",
    "            style_v = style_loader.get(batch_id)\n",
    "            style_model.setTarget(style_v)\n",
    "\n",
    "            features_style = vgg(style_v)\n",
    "            gram_style = [gram_matrix(y) for y in features_style]\n",
    "\n",
    "            y = style_model(x)\n",
    "            xc = Variable(x.data.clone())\n",
    "\n",
    "            features_y = vgg(y)\n",
    "            features_xc = vgg(xc)\n",
    "\n",
    "            f_xc_c = Variable(features_xc[1].data, requires_grad=False)\n",
    "\n",
    "            content_loss = content_weight * mse_loss(features_y[1], f_xc_c)\n",
    "\n",
    "            style_loss = 0.\n",
    "            for m in range(len(features_y)):\n",
    "                gram_y = gram_matrix(features_y[m])\n",
    "                gram_s = Variable(gram_style[m].data, requires_grad=False).repeat(n_batch, 1, 1)\n",
    "                style_loss += style_weight * mse_loss(gram_y, gram_s)\n",
    "\n",
    "            total_loss = content_loss + style_loss\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            agg_content_loss += content_loss.data\n",
    "            agg_style_loss += style_loss.data\n",
    "\n",
    "            if (batch_id + 1) % 23 == 0:\n",
    "                from IPython.display import clear_output\n",
    "                clear_output(wait=True)\n",
    "                mesg = \"{} | Epoch {}: [{}/{}] | content: {:.3f}/{:.3f}, style: {:.3f}/{:.3f}, total: {:.3f}\".format(\n",
    "                    time.ctime(), e + 1, count, len(train_dataset),\n",
    "                                content_loss.item(), agg_content_loss / (batch_id + 1),\n",
    "                                style_loss.item(), agg_style_loss / (batch_id + 1),\n",
    "                                (agg_content_loss + agg_style_loss) / (batch_id + 1)\n",
    "                )\n",
    "                tbar.set_description(mesg)\n",
    "                grid_show([(style_v[0].detach()[:,::2,::2], 'Стиль'), (x[0].detach(), 'Контент'), (y[0].detach(), 'Результат')])\n",
    "                plt.show()\n",
    "                \n",
    "            if (batch_id + 1) % 10000 == 0:\n",
    "                style_model.eval()\n",
    "                style_model = style_model.cpu()\n",
    "                torch.save(style_model.state_dict(), 'msgnet_checkpoint.pth')\n",
    "                style_model = style_model.cuda()\n",
    "                style_model.train()\n",
    "\n",
    "            \n",
    "    style_model.eval()\n",
    "    style_model.cpu()\n",
    "    save_model_path = 'msgnet.pth'\n",
    "    torch.save(style_model.state_dict(), save_model_path)\n",
    "\n",
    "    print(\"\\nВсё получилось! ;-) Модель здесь:\", save_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5b1842",
   "metadata": {},
   "outputs": [],
   "source": [
    "train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
